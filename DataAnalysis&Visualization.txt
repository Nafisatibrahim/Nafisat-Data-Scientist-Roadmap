I. Data Analysis and Visualization
Tools and technicques utilized to  explore data and discover hidden patterns using visualization. 

  🔴 1. Basics of Linear Algebra
Matrix: A matrix of size m x n is a two-dimensional array that has m rows and n columns.
Vector: A vector is a one-dimensional array that has only one row and one column.
Addition and Substractio: element-wise
Multiplying a matrix with a vector: To multiply a matrix with a column (or row) vector, the matrix must have the same number of columns (or rows) as the number of elements in the column (or row) vector.
Multiplying a matrix with another matrix: 
  - Dot Product
  - Cross Product
Determinant of a matrix:The determinant of a matrix is a special number that can only be calculated from a square matrix, i.e., a matrix with the same number of rows and columns. The determinant is calculated using the below formula:
 det(A)= a*d - b*c
<img src="square-matrix.jpg" alt="Sqaure" width="50" height="50">

Note: In Python, vectors and matrices are represented as Numpy arrays. Numpy is one of the most popular linear algebra packages in Python and can be used to implement several basic mathematical operations on matrices, such as calculating dot products and determinants.

  🔴 2. Eigenvectors and Eigenvalues
Linear transformation
Eigenvectors: A vector that keeps its direction when a linear transformation is applied.
Eigenvalue:  The value with which the eigenvector gets scaled, is known as the Eigenvalue,
Note: If a matrix is not invertible then the determinant of the matrix is equal to zero.

  🔴 3. The Covariance Matrix
Variance: Variance helps us understand how far our random variable is spread out from the mean.
Covariance: Covariance, on the other hand, measures the extent to which two random variables vary together.
Note: The function numpy.cov() in Python's Numpy variable can be used to get the covariance matrix in Python.

  🔴 4. Dimensionality Reduction (reduce number of columns or features)
Two important techniques:
  - PCA
  - t-SNE
PCA (Principle Component Analysis): the main idea is to reduce the dimensionality of a dataset consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset.
This is done by transforming the variables to a new coordinate space of variables, which are known as principal components (or simply, the PCs), and are orthogonal to each other.
The selection of principal components is such that the retention of variation present in the original variables is the maximum for the first principal component and decreases as we move down in the order. The principal components are the eigenvectors of the covariance matrix, and hence they are orthogonal.
t-SNE: this algorithm calculates a similarity measure between pairs of instances in the high dimensional space and in the low dimensional space. This is done by setting the probabilities from the low-dimensional space similar to those of the high dimensional space. We measure the difference between the probability distributions of the two-dimensional spaces using Kullback-Leibler divergence and try to optimize it.

  🔴 5. Kullback-Leibler (KL) Divergence
Sometimes, in machine learning, it is desirable to analyze the actual and observed probability distribution to quantify the difference in the distributions of a random variable. We calculate KL divergence to measure that difference.
Divergence: is a measure of how one distribution differs from another. It is not symmetrical in nature i.e. score of the divergence for distributions p and q would give a different score from q and p.
KL Divergence: It quantifies how much one probability distribution differs from another probability distribution.
Two important points to note about KL divergence:
  a. The lower the KL divergence value, the better the two distributions match. If two distributions perfectly match, then it is zero.
  b. It is not symmetrical
Note: In Python, we use the rel_entr function to calculate the KL divergence.

  🔴 6. The Binomial and Bernoulli Distributions
A Bernoulli trial (or a Binomial trial) is a random experiment with exactly 2 possible outcomes, “success” or “failure”, in which the probability of success is the same every time the experiment is conducted. 
There are some of the assumptions that need to be satisfied for a random variable to follow a binomial distribution. They are as follows: 
  1. There are a fixed number of trials. (represented by the variable n)
  2. Each trial has 2 outcomes (called "success" and "failure" for convenience)
  3. The trials are all independent of one another.
  4. The probability of success, i.e., p is the same for all trials.

II. Network Analysis
  🔴 1. Power-Law Distribution
Power-Law Distribution: is a term used when there are only a small number of observations which have a very high value of a certain characteristic, and a large number of observations which have a small value for that characteristic. Many unequally-distributed phenomena in the real world follow such a distribution.
Graph Theory: is the study of graphical structures that model relations between two variables or objects. Structurally, graphs are merely a collection of nodes inter-connected by edges in various ways.
Graphs are used by various algorithms in machine learning to perform tasks like clustering, classification, and regression.
A graph is usually represented using nodes (or vertices) and edges (or links).

  🔴 2. Matrix Representation of Graphs
An adjacency matrix is the matrix representation of a graph/network, depicting the connections between the nodes in the graph merely as numbers of a matrix. This equivalence between graphs and matrices is an important part of modern graph theory.
Weighted Adjacency Matrix: We use the weighted adjacency matrix in the case of a weighted directed or weighted undirected graph. A weighted graph is a type of graph where each link/edge is also given a numerical weight.

III. Unsupervised Learning
  🔴 1. Unsupevised Learning
Data Science and machine learning algorithmic techniques can be divided into three categories:
  1. Supervised learning: The algorithm has prior data (labeled data) to learn from.
  2. Unsupervised learning: The algorithm has no prior labeled data to learn from i.e., it learns from unlabeled data by finding patterns among examples and grouping them accordingly.
  3. Reinforcement learning:The algorithm learns patterns or behaviors in a recursive manner by taking actions to maximize a certain reward, and uses feedback from the environment to learn the best action to take under the right circumstances in order to maximize that reward.

Unsupervised Learning is a category of techniques that train computers to use a set of unlabeled / unseen data and learn by themselves. The algorithm is provided with a large volume of data and expected to identify hidden patterns. Since there's no prior information / label available for each data instance, there is no defined / correct outcome. The machines only need to determine if there are any patterns in the given data.
Unsupervised Learning is utilised in the following use cases:
  1. Clustering: the key idea is to divide the data into groups such that each group shares similar properties and each group is as dissimilar as possible to the other groups.
  2. Association
  3. Dimensionality Reduction 

  🔴 2. Distance and Scaling Measures
Unsupervised learning algorithms use different distance measures or similarity / dissimilarity measures between each pair of observations to group data into different clusters. Points which are close to each other on that distance metric are likely to be grouped into the same cluster, while points far apart on that distance metric are likely to be members of different clusters. All this is done by computing a distance matrix which has distances between every pair of observations. There are different ways of calculating these distances, some of which are mentioned below:
  1. Euclidean distance: Euclidean distance is calculated as the square root of the sum of the squared differences b/w two vectors.
  2. Manhattan distance: Also called city block distance, it calculates the distance between two points by drawing an orthogonal, zig-zag grid between them.

Scaling: 
Normalization: One of the ways of scaling the data is so that all the values lie between 0 and 1. This is called Normalization. A value can be normalized as follows:

                        y = (x–min) / (max-min)
where,
y: normalized version of the variable
x: variable of interest
min: minimum value of the variable x in this dataset
max: maximum value of the variable x in this dataset

Standardization:  is another way of scaling the data, where the mean of the observations becomes 0 and the standard deviation is 1.  A value can be standardized as follows:

                        y = (x-mean) / std_dev

where,
y: standardized version of the variable
x: variable of interest
mean: average (arithmetic mean) of the variable x in the dataset
std_dev: standard deviation of the variable x in the dataset

  🔴 3. K-Means Clustering
K-means Clustering is an unsupervised learning algorithm. Like other clustering algorithms, it tries to aggregate similar objects into groups called clusters.
In K-means Clustering, K refers to the number of clusters required. The concept of a centroid, which is the geometric center of a cluster, is used to determine the clusters that K-means finds in the dataset.

Working of K-Means Clustering: The steps involved in K-means Clustering are:
  1. Choose the number of clusters K
  2. Initialize the centroids 
  3. Assign each data point to the closest centroid.
  4. Update the centroid by taking the mean of the cluster.
-> Repeat steps 3 and 4 until convergence i.e No discernible change in centroids is observed.
-> For step 3, to assign each data point to the nearest centroid, we use the distance between the centroid and the data point. This distance can be found using Euclidean distance.
Therefore, K-means clustering uses the Euclidean distance to allocate each of the data points to its nearest cluster, so that the sum of squares within each cluster is minimum.

Live Session: Data Collection
What to consider when collecting data?
-> Perform a controlled randomized experiment to minimize the problem of confounding

Evaluation Methods: For K-means clustering we need a fixed value of K, and that should be known before performing the method. It is not a learned parameter, and we will have to figure it out. To find out the optimal K number, one method we can use is called the ELBOW method.

ELBOW Method:
In this method, we take different values of K i.e from 1 to the range required. For each value of K, we calculate the sum of squares within clusters, usually called WCSS (Within Cluster Sum of Squares). So we calculate the sum of squared distance between the centroids and the data points for each cluster, and do a summation of all the clusters. Then, we plot WCSS vs K - the plot shape looks similar to that of an elbow. As we see in the graph, as K increases the WCSS is going to decrease. The optimal point is at the elbow - after that as K increases, the fall in WCSS is not that significant. In the given example, K=7 appears to be the optimal value of K.

Assumptions:
1. K-means Clustering is limited to linear cluster boundaries: The fundamental model assumptions of K-means Clustering (points will be closer to their cluster center than to others), means that the algorithm will often be ineffective if the clusters have complicated geometries.
2. All clusters are of the same size.
3. Clusters have the same extent in every direction. This assumption would not be true in a dataset where different measurements are in different units.
4. Clusters have similar numbers of points assigned to them.

Applications:
Document Clustering - Group similar documents together
Customer Segmentation - Divide customers into similar groups
Image Segmentation - Grouping similar pixels together

  🔴 4. Gaussian Mixture Models (need to review, didn't undertand it)

  🔴 5. Hierarchical Clustering
Hierarchical Clustering is one of the many types of clustering algorithms. 

There are 2 types of hierarchical clustering:
  1. Agglomerative clustering 
  2. Divisive clustering 

------Agglomerative Clustering:------ 
It is a bottom-up clustering method.
Step 1: Let’s say we have N data points, then each of the points will be considered as N different clusters.
Step 2: Find two clusters that are closest among the N clusters (from step 1) and combine them into a single cluster. This will result in having N-1 clusters now.
Step 3: Find the next 2 closest clusters and combine them into a single cluster and we have N-2 clusters now. This will be continued till we have only one cluster containing all the data points. The below graphic demonstrates this process.

To calculate the distance between two clusters, we use the linkage method. There are different types of linkages that can be used to compare the distance between two clusters:
a. Complete-linkage: The distance between two clusters is defined as the longest distance between two points in each cluster.  
b. Average-linkage: The distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. Average-linkage and complete-linkage are the two most popular distance metrics in hierarchical clustering.
c. Centroid-linkage: The distance between the centroids of two clusters. It finds the centroid of cluster 1 and centroid of cluster 2 and then calculates the distance between the two before merging.
d. Single-linkage: The distance between two clusters is defined as the shortest distance between two points in each cluster. This linkage may be used to detect high values in your dataset which may be outliers as they will be merged at the end. 

------Divisive Clutsering------
In contrast to Agglomerative clustering, divisive clustering is a top-down clustering method. We work backward in this method.
Step 1: All the points are grouped together into a single cluster.
Step 2: The single cluster is split into two different clusters. 
Step 3: Step 2 is repeated till the number of clusters is equal to the number of data points.  

Dendrogram
A dendrogram is a diagram that shows the hierarchical relationship between objects. It is used to see the output of hierarchical clustering. The below graphic explains, how the dendrogram is being formed as the clustering happens.

  🔴 6. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
DBSCAN is another type of clustering algorithm. It works based on the concept of density difference. Density is interpreted as the concentration of the samples present in a region. The algorithm differentiates between high and low-density regions and clusters the points accordingly. 
There are two parameters to the algorithm which is used to define the term dense.
- epsilon (eps): It specifies how close points should be to each other to be considered as part of a cluster. Two points are considered neighbors if the distance between the two points is below the threshold epsilon.
- minPoints: the minimum number of points to form a dense region. For example, if we set the minPoints parameter as 4, then we need at least 4 points to form a dense region.


